"""
ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å‹è‹±æ–‡ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
Approach 3: ML Model + LLM Integration
"""

import os
import random
import re
import itertools
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
import json
import pickle
from datetime import datetime

# ----------------------------------------
# å°ã•ãªãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ----------------------------------------
def _slug(s: str) -> str:
    t = s.strip().lower()
    t = re.sub(r"[^a-z0-9\- _]+", "", t)
    t = t.replace(" ", "_")
    t = re.sub(r"_+", "_", t)
    return t or "text"

def _norm_user_path(p: str) -> str:
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚·ã‚§ãƒ«é¢¨ã«ç©ºç™½ã‚’ \ ã§ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ãŸå…¥åŠ›ã‚’æ­£è¦åŒ–
    if not p:
        return p
    p = p.strip().strip('"').strip("'")
    p = p.replace('\\ ', ' ')
    return p

# ----------------------------------------
# å›ºå®šãƒ‘ã‚¹ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®çµ¶å¯¾ãƒ‘ã‚¹ã‚’ã‚³ãƒ¼ãƒ‰ã«çµ„ã¿è¾¼ã¿ï¼‰
# ----------------------------------------
DEFAULT_EXCEL_PATH = \
    "/Users/ryuya/Library/CloudStorage/Dropbox/ç§‘ç ”_CAT/Material/Text_Generation/CVLA3_20250912133649_3373.xlsx"
DEFAULT_WORDLIST_PATH = \
    "/Users/ryuya/Library/CloudStorage/Dropbox/ç§‘ç ”_CAT/Material/Text_Generation/CEFR-J Wordlist Ver1.6.xlsx"
DEFAULT_READING_TEXT_DIR = \
    "/Users/ryuya/Library/CloudStorage/Dropbox/ç§‘ç ”_CAT/Material/Reading_Text"

# ãƒ¢ãƒ‡ãƒ«ã§ç”¨ã„ã‚‹ç‰¹å¾´é‡ã®åˆ—åï¼ˆçµ±ä¸€å®šç¾©ï¼‰
FEATURE_NAMES = ['ARI', 'AvrDiff', 'BperA', 'CVV1', 'AvrFreqRank', 'VperSent', 'POStypes', 'LenNP']

# ========================================
# STEP 1: APIè¨­å®š
# ========================================

class APIConfiguration:
    """OpenAI APIè¨­å®šã®ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆOpenAIå°‚ç”¨ï¼‰"""
    
    def __init__(self):
        self.config = {}
        self.setup_apis()
    
    def setup_apis(self):
        """OpenAIã®ã¿è¨­å®š"""
        self.config['openai'] = {
            'api_key': os.getenv('OPENAI_API_KEY', ''),
            'model': 'gpt-4',  # äºˆç®—ã«å¿œã˜ã¦å¤‰æ›´å¯èƒ½
            'endpoint': 'https://api.openai.com/v1/chat/completions',
            'cost_per_1k_tokens': {
                'gpt-4': {'input': 0.03, 'output': 0.06},
                'gpt-3.5-turbo': {'input': 0.0015, 'output': 0.002}
            }
        }
    
    def get_api_key(self) -> str:
        """OpenAI APIã‚­ãƒ¼ã‚’å–å¾—"""
        return self.config['openai'].get('api_key', '')
    
    def estimate_cost(self, input_tokens: int, output_tokens: int, model: Optional[str] = None) -> float:
        """OpenAIã®ã‚³ã‚¹ãƒˆã‚’æ¨å®š"""
        model = model or self.config['openai'].get('model', '')
        costs = self.config['openai'].get('cost_per_1k_tokens', {})
        if model in costs:
            input_cost = (input_tokens / 1000) * costs[model]['input']
            output_cost = (output_tokens / 1000) * costs[model]['output']
            return input_cost + output_cost
        return 0.0

# ========================================
# STEP 2: APIåˆ¥ã®å®Ÿè£…
# ========================================

class LLMProvider:
    """OpenAIå°‚ç”¨ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å®Ÿè£…"""
    
    def __init__(self):
        self.api_config = APIConfiguration()
        self.setup_provider()
    
    def setup_provider(self):
        """OpenAIã®åˆæœŸè¨­å®š"""
        api_key = self.api_config.get_api_key()
        self._setup_openai(api_key)
    
    def _setup_openai(self, api_key: str):
        """OpenAI APIã®è¨­å®š"""
        try:
            import openai
            from openai import OpenAI
            
            if not api_key:
                print("âš ï¸ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                print("è¨­å®šæ–¹æ³•:")
                print("1. https://platform.openai.com/api-keys ã§APIã‚­ãƒ¼ã‚’å–å¾—")
                print("2. ç’°å¢ƒå¤‰æ•°ã«è¨­å®š: export OPENAI_API_KEY='your-key-here'")
                print("3. ã¾ãŸã¯ç›´æ¥æŒ‡å®š: client = OpenAI(api_key='your-key-here')")
                return None
            
            self.client = OpenAI(api_key=api_key)
            print("âœ… OpenAI APIè¨­å®šå®Œäº†")
            
        except ImportError as e:
            print(f"OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            print("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«/æ›´æ–°ã‚³ãƒãƒ³ãƒ‰: pip install -U openai")
    
    def generate(self, prompt: str, max_tokens: int = 500, temperature: float = 0.7) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
        return self._generate_openai(prompt, max_tokens, temperature)
    
    def _generate_openai(self, prompt: str, max_tokens: int, temperature: float) -> str:
        """OpenAIã§ã®ç”Ÿæˆ"""
        if not hasattr(self, 'client'):
            return "[OpenAI APIæœªè¨­å®š] ãƒ‡ãƒ¢ãƒ†ã‚­ã‚¹ãƒˆ"
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are an expert in creating CEFR-leveled English texts."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=temperature
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def classify_level(self, text: str, candidate_levels: Optional[List[str]] = None) -> Tuple[str, float]:
        """OpenAIã«CEFRãƒ¬ãƒ™ãƒ«åˆ¤å®šã‚’ä¾é ¼ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
        if not hasattr(self, 'client'):
            return "unknown", 0.0
        levels = candidate_levels or ['A1.1','A1.2','A2.2','B1.1','B1.2','B2.1','B2.2','C1','C2']
        try:
            prompt = (
                "Classify the CEFR level of the following English text.\n"
                f"Choose one EXACTLY from: {', '.join(levels)}.\n"
                "Respond in JSON: {\"level\": <one of levels>, \"confidence\": <0-1 float>}\n\n"
                f"Text:\n{text}"
            )
            resp = self.client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a precise CEFR evaluator."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=200,
                temperature=0.0
            )
            content = resp.choices[0].message.content.strip()
            data = json.loads(content)
            level = str(data.get("level", "unknown"))
            conf = float(data.get("confidence", 0.0))
            if level not in levels:
                return "unknown", conf
            return level, conf
        except Exception:
            return "unknown", 0.0
    

# ========================================
# STEP 3: MLãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…
# ========================================

class ReadabilityPredictor:
    """èª­ã¿ã‚„ã™ã•ãƒ¬ãƒ™ãƒ«äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self, excel_path: str):
        self.df = None
        if excel_path and os.path.exists(excel_path):
            try:
                self.df = pd.read_excel(excel_path)
            except Exception as e:
                print(f"âš ï¸ Excelã®èª­ã¿è¾¼ã¿ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                self.df = None
        else:
            if excel_path:
                print(f"âš ï¸ æŒ‡å®šã•ã‚ŒãŸExcelãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {excel_path}")
            else:
                print("â„¹ï¸ Excelãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
        self.model = None
        self.scaler = None
        self.label_encoder = None
        self.feature_names = list(FEATURE_NAMES)
    
    def train_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
        if self.df is None:
            print("â­ï¸ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return 0.0

        # å¿…é ˆã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèª
        required_cols = set(self.feature_names + ['predict_level'])
        missing = [c for c in required_cols if c not in self.df.columns]
        if missing:
            print(f"âš ï¸ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing}")
            print("â­ï¸ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return 0.0
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.preprocessing import StandardScaler, LabelEncoder
        from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
        from sklearn.linear_model import LogisticRegression
        from sklearn.calibration import CalibratedClassifierCV
        
        print("ğŸ“Š MLãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        X = self.df[self.feature_names].values
        y = self.df['predict_level'].values
        
        # ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        self.label_encoder = LabelEncoder()
        y_encoded = self.label_encoder.fit_transform(y)
        
        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã‚’ç¢ºèª
        unique, counts = np.unique(y_encoded, return_counts=True)
        min_class_count = counts.min() if len(counts) else 0
        use_stratify = min_class_count >= 2
        if not use_stratify:
            print("âš ï¸ ä¸€éƒ¨ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒ1ä»¶ã®ãŸã‚ã€å±¤åŒ–åˆ†å‰²ã‚’ç„¡åŠ¹åŒ–ã—ã¾ã™ã€‚")
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, test_size=0.2, random_state=42,
            stratify=y_encoded if use_stratify else None
        )
        
        # ç‰¹å¾´é‡ã®æ­£è¦åŒ–
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # å€™è£œãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã—ã€ç¢ºç‡è¼ƒæ­£ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¡ç”¨
        rf = RandomForestClassifier(
            n_estimators=300,
            max_depth=None,
            class_weight='balanced',
            random_state=42,
            n_jobs=-1
        )
        lr = LogisticRegression(
            max_iter=2000,
            multi_class='ovr',
            class_weight='balanced',
            solver='liblinear'
        )
        base_model = rf
        try:
            # cvåˆ†å‰²æ•°ã¯å„ã‚¯ãƒ©ã‚¹ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ã«ä¾å­˜
            n_splits = min(5, int(min_class_count))
            if n_splits >= 2 and use_stratify:
                cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
                scores_rf = cross_val_score(rf, X_train_scaled, y_train, cv=cv)
                scores_lr = cross_val_score(lr, X_train_scaled, y_train, cv=cv)
                base_model = rf if scores_rf.mean() >= scores_lr.mean() else lr
                chosen = 'RF' if base_model is rf else 'LR'
                print(f"ğŸ” åŸºæœ¬ãƒ¢ãƒ‡ãƒ«é¸æŠ: {chosen} (cv={max(scores_rf.mean(), scores_lr.mean()):.2%})")
            else:
                print("â„¹ï¸ ã‚¯ãƒ©ã‚¹æ•°ãŒå°‘ãªã„ãŸã‚CVè©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€RFã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        except Exception as e:
            print(f"â„¹ï¸ CVè©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç†ç”±: {e}ï¼‰ã€‚RFã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            base_model = rf

        base_model.fit(X_train_scaled, y_train)
        # ç¢ºç‡è¼ƒæ­£ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒååˆ†ãªã¨ãï¼‰
        try:
            calib_splits = max(2, min(3, int(min_class_count))) if use_stratify else 2
            if calib_splits >= 2:
                self.model = CalibratedClassifierCV(base_model, method='isotonic', cv=calib_splits)
                self.model.fit(X_train_scaled, y_train)
            else:
                self.model = base_model
                print("â„¹ï¸ ã‚µãƒ³ãƒ—ãƒ«ä¸è¶³ã®ãŸã‚ç¢ºç‡è¼ƒæ­£ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        except Exception:
            self.model = base_model
        
        # ç²¾åº¦è©•ä¾¡
        from sklearn.metrics import accuracy_score, classification_report
        y_pred = self.model.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)
        
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†ï¼ç²¾åº¦: {accuracy:.2%}")
        
        # ç‰¹å¾´é‡ã®é‡è¦åº¦
        importance = pd.DataFrame({
            'feature': self.feature_names,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("\nğŸ“ˆ ç‰¹å¾´é‡ã®é‡è¦åº¦:")
        for _, row in importance.head(5).iterrows():
            print(f"  - {row['feature']}: {row['importance']:.3f}")
        
        return accuracy
    
    def predict_level(self, features: Dict[str, float]) -> Tuple[str, float]:
        """ãƒ¬ãƒ™ãƒ«ã‚’äºˆæ¸¬"""
        if not self.model:
            return "B1.1", 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        # ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆ
        X = np.array([[features.get(f, 0) for f in self.feature_names]])
        X_scaled = self.scaler.transform(X)
        
        # äºˆæ¸¬
        y_pred = self.model.predict(X_scaled)[0]
        y_proba = self.model.predict_proba(X_scaled)[0]
        
        level = self.label_encoder.inverse_transform([y_pred])[0]
        confidence = max(y_proba)
        
        return level, confidence
    
    def save_model(self, filepath: str = 'readability_model.pkl'):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        with open(filepath, 'wb') as f:
            pickle.dump({
                'model': self.model,
                'scaler': self.scaler,
                'label_encoder': self.label_encoder,
                'feature_names': self.feature_names
            }, f)
        print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ {filepath} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    def load_model(self, filepath: str = 'readability_model.pkl'):
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
            self.model = data['model']
            self.scaler = data['scaler']
            self.label_encoder = data['label_encoder']
            self.feature_names = data['feature_names']
        print(f"ğŸ“‚ ãƒ¢ãƒ‡ãƒ«ã‚’ {filepath} ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

# ========================================
# STEP 4: ç‰¹å¾´é‡æŠ½å‡º
# ========================================

class FeatureExtractor:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆCEFR-J Wordlistå¯¾å¿œï¼‰"""
    
    def __init__(self, cefr_wordlist_path: Optional[str] = None):
        self.cefr_wordlist_path = cefr_wordlist_path or os.getenv('CEFR_WORDLIST_PATH', '')
        self.word_level_map: Dict[str, str] = {}
        self.setup_tools()
        # Wordlist è‡ªå‹•æ¤œå‡º
        if not self.cefr_wordlist_path:
            for c in [
                'CEFR-J Wordlist Ver1.6.xlsx',
                'CEFR-J Wordlist.xlsx',
                'CEFR_J_Wordlist.xlsx'
            ]:
                if os.path.exists(c):
                    self.cefr_wordlist_path = c
                    break
        if self.cefr_wordlist_path and os.path.exists(self.cefr_wordlist_path):
            self._load_cefr_wordlist(self.cefr_wordlist_path)
        else:
            if self.cefr_wordlist_path:
                print(f"âš ï¸ CEFR-J WordlistãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.cefr_wordlist_path}")
            else:
                print("â„¹ï¸ CEFR-J Wordlistã®ãƒ‘ã‚¹ãŒæœªæŒ‡å®šã§ã™ï¼ˆç’°å¢ƒå¤‰æ•° CEFR_WORDLIST_PATH ã§æŒ‡å®šå¯ï¼‰")
    
    def setup_tools(self):
        """å¿…è¦ãªãƒ„ãƒ¼ãƒ«ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            import textstat
            import spacy
            
            # spaCyãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            try:
                self.nlp = spacy.load("en_core_web_sm")
            except:
                print("spaCyãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
                os.system("python -m spacy download en_core_web_sm")
                self.nlp = spacy.load("en_core_web_sm")
            
            self.textstat = textstat
            # ä»»æ„: é »åº¦æ¨å®šç”¨ã® wordfreq
            try:
                import wordfreq as _wf
                self.wordfreq = _wf
            except Exception:
                self.wordfreq = None
                print("â„¹ï¸ wordfreq æœªå°å…¥ã®ãŸã‚ AvrFreqRank ã¯è¿‘ä¼¼å€¤ã«ãªã‚Šã¾ã™ï¼ˆpip install wordfreq ã§å°å…¥å¯ï¼‰")
            print("âœ… ç‰¹å¾´é‡æŠ½å‡ºãƒ„ãƒ¼ãƒ«æº–å‚™å®Œäº†")
            
        except ImportError as e:
            print(f"å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„: {e}")
            print("pip install textstat spacy")
    
    def _load_cefr_wordlist(self, path: str):
        """CEFR-J Wordlistï¼ˆExcelï¼‰ã‚’èª­ã¿è¾¼ã¿ã€è¦‹å‡ºã—èªâ†’ãƒ¬ãƒ™ãƒ«ã®è¾æ›¸ã‚’ä½œæˆ"""
        try:
            xls = pd.ExcelFile(path)
        except Exception as e:
            print(f"âš ï¸ CEFR-J Wordlistã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            return
        # ã™ã¹ã¦ã®ã‚·ãƒ¼ãƒˆã‚’èµ°æŸ»ã—ã¦å€™è£œåˆ—ã‚’æ¢ç´¢
        candidates = []
        for sheet in xls.sheet_names:
            try:
                df = xls.parse(sheet)
            except Exception:
                continue
            lower = {c.lower(): c for c in df.columns}
            def find_col(keys):
                for k in keys:
                    if k in lower:
                        return lower[k]
                return None
            word_col = find_col(['headword','lemma','lemmas','word','å˜èª','è¦‹å‡ºã—èª'])
            level_col = find_col(['cefr-j','cefr','level','ãƒ¬ãƒ™ãƒ«','cefr level','cefr-j level','level (cefr)'])
            if word_col and level_col:
                candidates.append((sheet, df, word_col, level_col))
        if not candidates:
            print("âš ï¸ CEFR-J Wordlistã®åˆ—åã‚’ã„ãšã‚Œã®ã‚·ãƒ¼ãƒˆã§ã‚‚æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚·ãƒ¼ãƒˆæ§‹æˆã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
            print(f" åˆ©ç”¨å¯èƒ½ã‚·ãƒ¼ãƒˆ: {xls.sheet_names}")
            return
        # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸå€™è£œã‚’ä½¿ç”¨
        sheet, df, word_col, level_col = candidates[0]
        def norm_word(w: str) -> str:
            return str(w).strip().lower()
        def norm_level(lbl: str) -> str:
            s = str(lbl).strip().upper().replace(' ', '')
            m = re.match(r'([ABC][12])', s)
            return m.group(1) if m else s
        mapping = {}
        for _, row in df[[word_col, level_col]].dropna().iterrows():
            w = norm_word(row[word_col])
            lv = norm_level(row[level_col])
            if w and lv:
                mapping[w] = lv
        self.word_level_map = mapping
        print(f"ğŸ“š CEFR-J Wordlistã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆã‚·ãƒ¼ãƒˆ: {sheet}ï¼‰: {len(self.word_level_map)}èª")
    
    def extract_features(self, text: str) -> Dict[str, float]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        
        if not hasattr(self, 'textstat'):
            # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
            return {
                'ARI': 5.0,
                'AvrDiff': 1.5,
                'BperA': 0.3,
                'CVV1': 4.0,
                'AvrFreqRank': 1500,
                'VperSent': 2.5,
                'POStypes': 8.0,
                'LenNP': 3.5
            }
        
        doc = self.nlp(text)
        
        # å„ç‰¹å¾´é‡ã‚’è¨ˆç®—
        features = {}
        
        # ARI (Automated Readability Index)
        features['ARI'] = self.textstat.automated_readability_index(text)
        
        # AvrDiffï¼ˆå†…å®¹èªã®ã¿ã€CEFR-Jãƒ¬ãƒ™ãƒ«ã®å¹³å‡: A1=1..C2=6ï¼‰
        content_pos = {"NOUN", "VERB", "ADJ", "ADV"}
        lv_nums = []
        for tok in doc:
            if tok.is_alpha and tok.pos_ in content_pos:
                lbl = self._lookup_level(tok.lemma_.lower())
                if lbl:
                    lv_nums.append(self._level_to_num(lbl))
        features['AvrDiff'] = float(np.mean(lv_nums)) if lv_nums else 0.0
        
        # BperAï¼ˆBãƒ¬ãƒ™ãƒ«/ Aãƒ¬ãƒ™ãƒ« ã®å†…å®¹èªæ¯”ï¼‰
        a_cnt = 0
        b_cnt = 0
        for tok in doc:
            if tok.is_alpha and tok.pos_ in content_pos:
                lbl = self._lookup_level(tok.lemma_.lower())
                if not lbl:
                    continue
                if lbl.startswith('A'):
                    a_cnt += 1
                elif lbl.startswith('B'):
                    b_cnt += 1
        features['BperA'] = (b_cnt / a_cnt) if a_cnt > 0 else 0.0
        
        # CVV1 = unique verbs / sqrt(2 * total verbs)
        verbs = [t for t in doc if t.pos_ == 'VERB']
        v_total = len(verbs)
        v_types = len(set(t.lemma_.lower() for t in verbs))
        features['CVV1'] = (v_types / np.sqrt(2 * v_total)) if v_total > 0 else 0.0
        
        # AvrFreqRankï¼ˆæ©Ÿèƒ½èªå«ã‚€ã€æœ€ã‚‚ä½é »åº¦ã®3èªã‚’é™¤å¤–ï¼‰
        if getattr(self, 'wordfreq', None):
            words = [t.text.lower() for t in doc if t.is_alpha]
            if words:
                zipfs = [(w, self.wordfreq.zipf_frequency(w, 'en')) for w in words]
                zipfs.sort(key=lambda x: x[1])  # ä½â†’é«˜
                trimmed = zipfs[3:] if len(zipfs) > 3 else zipfs
                ranks = [10.0 - z for _, z in trimmed]  # å°ã•ã„ã»ã©é«˜é »åº¦
                features['AvrFreqRank'] = float(np.mean(ranks)) if ranks else 0.0
            else:
                features['AvrFreqRank'] = 0.0
        else:
            features['AvrFreqRank'] = max(0.0, 10.0 - features['ARI'])
        
        # æ–‡ã‚ãŸã‚Šã®å‹•è©æ•°
        sentences = list(doc.sents)
        features['VperSent'] = (len(verbs) / len(sentences)) if sentences else 0.0
        
        # å“è©ã‚¿ã‚¤ãƒ—æ•°
        pos_types = set([token.pos_ for token in doc])
        features['POStypes'] = float(len(pos_types))
        
        # LenNPï¼ˆæ–‡ã‚ãŸã‚Šã®åè©å¥é•·åˆè¨ˆã®å¹³å‡ï¼‰
        if sentences:
            per_sent_np_len = []
            for sent in sentences:
                nps = list(sent.noun_chunks)
                total_len = sum(len(np_.text.split()) for np_ in nps)
                per_sent_np_len.append(total_len)
            features['LenNP'] = float(np.mean(per_sent_np_len)) if per_sent_np_len else 0.0
        else:
            features['LenNP'] = 0.0
        
        return features

    # --- è¾æ›¸ãƒ»ãƒ¬ãƒ™ãƒ«è£œåŠ© ---
    def _lookup_level(self, lemma_lower: str) -> Optional[str]:
        if not self.word_level_map:
            return None
        return self.word_level_map.get(lemma_lower)
    
    @staticmethod
    def _level_to_num(lbl: str) -> int:
        s = lbl.upper().strip().replace(' ', '')
        m = re.match(r'([ABC][12])', s)
        key = m.group(1) if m else s
        table = {'A1':1, 'A2':2, 'B1':3, 'B2':4, 'C1':5, 'C2':6}
        return table.get(key, 0)

# ========================================
# STEP 5: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼
# ========================================

class HybridTextGenerator:
    """MLãƒ¢ãƒ‡ãƒ«ã¨LLMã‚’çµ„ã¿åˆã‚ã›ãŸç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, 
                 excel_path: str,
                 api_key: Optional[str] = None,
                 cefr_wordlist_path: Optional[str] = None):
        """
        Parameters:
        -----------
        excel_path: èª­ã¿ã‚„ã™ã•ãƒ‡ãƒ¼ã‚¿ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        api_key: OpenAI APIã‚­ãƒ¼ï¼ˆç’°å¢ƒå¤‰æ•°ã§è¨­å®šæ¸ˆã¿ã®å ´åˆã¯ä¸è¦ï¼‰
        """
        
        print("ğŸš€ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ä¸­...")
        
        # ã¾ãšç‰¹å¾´æŠ½å‡ºå™¨ï¼ˆCEFRèªå½™è¡¨è¾¼ã¿ï¼‰ã‚’åˆæœŸåŒ–
        self.extractor = FeatureExtractor(cefr_wordlist_path)

        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•æ‹¡å……ï¼ˆReading_Text -> ç‰¹å¾´é‡ -> æ—¢å­˜Excelã«ãƒãƒ¼ã‚¸ï¼‰
        merged_excel_path = self._maybe_build_and_merge_training_data(
            base_excel_path=excel_path,
            text_dir=DEFAULT_READING_TEXT_DIR
        )

        # äºˆæ¸¬å™¨ã®åˆæœŸåŒ–ï¼ˆæ‹¡å……æ¸ˆã¿ãŒã‚ã‚Œã°ãã¡ã‚‰ã‚’ä½¿ç”¨ï¼‰
        self.predictor = ReadabilityPredictor(merged_excel_path or excel_path)
        
        # APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šï¼ˆæŒ‡å®šã•ã‚ŒãŸå ´åˆï¼‰
        if api_key:
            os.environ['OPENAI_API_KEY'] = api_key
        
        self.llm = LLMProvider()
        
        # MLãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
        self.predictor.train_model()
        
        print("âœ… ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†ï¼")

    def _maybe_build_and_merge_training_data(self, base_excel_path: str, text_dir: str) -> Optional[str]:
        """Reading_Text ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ç‰¹å¾´é‡ã‚’ä½œæˆã—ã€æ—¢å­˜Excelã«ãƒãƒ¼ã‚¸ã—ã¦ä¿å­˜ã€‚
        æˆ»ã‚Šå€¤: ç”Ÿæˆã•ã‚ŒãŸExcelãƒ‘ã‚¹ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
        """
        try:
            if not os.path.isdir(text_dir):
                return None
            if not (base_excel_path and os.path.exists(base_excel_path)):
                print("â„¹ï¸ æ—¢å­˜ã®å­¦ç¿’ExcelãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒãƒ¼ã‚¸å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return None
            print("ğŸ§ª å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æ‹¡å……ä¸­: Reading_Text -> ç‰¹å¾´é‡ -> æ—¢å­˜Excelã«ãƒãƒ¼ã‚¸")
            import glob
            txt_files = sorted(glob.glob(os.path.join(text_dir, "*.txt")))
            if not txt_files:
                print("â„¹ï¸ Reading_Text ã« .txt ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return None
            # æ—¢å­˜Excelã®èª­ã¿è¾¼ã¿
            base_df = pd.read_excel(base_excel_path)
            # ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã®æ¨å®š
            label_map = self._build_label_map_from_excel(base_df)
            if not label_map:
                print("â„¹ï¸ æ—¢å­˜Excelã‹ã‚‰ 'Text_#' ã¸ã®ãƒ©ãƒ™ãƒ«å¯¾å¿œã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return None
            # ãƒ†ã‚­ã‚¹ãƒˆâ†’ç‰¹å¾´é‡
            rows = []
            try:
                from tqdm import tqdm
                iterator = tqdm(txt_files, desc="Featureizing", ncols=80)
            except Exception:
                iterator = txt_files
            for path in iterator:
                try:
                    with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                        text = f.read()
                except Exception:
                    continue
                feats = self.extractor.extract_features(text)
                fname = os.path.basename(path)
                text_id = self._normalize_text_id(fname)
                label = label_map.get(text_id)
                if not label:
                    continue  # ãƒ©ãƒ™ãƒ«ãªã—ã¯å­¦ç¿’ã«ä½¿ã‚ãªã„
                feats['predict_level'] = label
                feats['filename'] = fname
                rows.append(feats)
            if not rows:
                print("â„¹ï¸ ãƒ©ãƒ™ãƒ«ä»˜ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç‰¹å¾´é‡ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                return None
            new_df = pd.DataFrame(rows)
            # ãƒãƒ¼ã‚¸ï¼ˆé‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ—¢å­˜å„ªå…ˆï¼‰
            if 'filename' in base_df.columns and 'filename' in new_df.columns:
                merged = pd.concat([base_df, new_df], ignore_index=True)
                merged = merged.sort_values(by=['filename']).drop_duplicates(subset=['filename'], keep='first')
            else:
                # filenameåˆ—ãŒç„¡ã„å ´åˆã¯å˜ç´”çµåˆï¼ˆåˆ—æ•´å½¢ï¼‰
                merged = pd.concat([base_df, new_df], ignore_index=True, sort=False)
            # å¿…è¦åˆ—ã®ä¸¦ã³
            cols = ['filename'] + list(FEATURE_NAMES) + ['predict_level']
            for c in cols:
                if c not in merged.columns:
                    merged[c] = None
            merged = merged[cols]
            out_dir = os.path.join('outputs')
            os.makedirs(out_dir, exist_ok=True)
            out_path = os.path.join(out_dir, 'training_dataset_merged.xlsx')
            merged.to_excel(out_path, index=False)
            print(f"âœ… å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ: {out_path} (è¡Œæ•°: {len(merged)})")
            return out_path
        except Exception as e:
            print(f"âš ï¸ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ‹¡å……å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    @staticmethod
    def _normalize_text_id(name: str) -> str:
        s = os.path.splitext(name)[0]
        s = s.strip().lower()
        m = re.search(r'text[_\s-]?(\d+)', s)
        return f"text_{m.group(1)}" if m else s

    @staticmethod
    def _build_label_map_from_excel(df: pd.DataFrame) -> Dict[str, str]:
        # å€™è£œã®åˆ—åã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«å/IDã‚’æ¢ã™
        lower = {c.lower(): c for c in df.columns}
        cand_keys = ['filename', 'file', 'text', 'text_name', 'name', 'id', 'text_id']
        name_col = None
        for k in cand_keys:
            if k in lower:
                name_col = lower[k]
                break
        if name_col is None:
            # å€¤ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ 'Text_#' ã‚’å«ã‚€åˆ—ã‚’æ¨å®š
            for c in df.columns:
                if df[c].astype(str).str.contains(r'Text[_\s-]?\d+', case=False, regex=True).any():
                    name_col = c
                    break
        if name_col is None or 'predict_level' not in df.columns:
            return {}
        mapping = {}
        for _, row in df[[name_col, 'predict_level']].dropna().iterrows():
            nid = HybridTextGenerator._normalize_text_id(str(row[name_col]))
            mapping[nid] = row['predict_level']
        return mapping
    
    def generate_with_validation(self,
                                target_level: str,
                                topic: str,
                                word_count: int = 200,
                                max_attempts: int = 5,
                                word_tolerance: float = 0.1) -> Dict:
        """
        ãƒ¬ãƒ™ãƒ«æ¤œè¨¼ä»˜ãã§ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
        
        Returns:
        --------
        {
            'text': ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ,
            'actual_level': äºˆæ¸¬ã•ã‚ŒãŸãƒ¬ãƒ™ãƒ«,
            'confidence': ä¿¡é ¼åº¦,
            'attempts': è©¦è¡Œå›æ•°,
            'features': ç‰¹å¾´é‡,
            'success': ç›®æ¨™ãƒ¬ãƒ™ãƒ«ã«åˆ°é”ã—ãŸã‹
        }
        """
        
        print(f"\nğŸ¯ ç›®æ¨™: {target_level}ãƒ¬ãƒ™ãƒ«ã®{word_count}èªã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒˆãƒ”ãƒƒã‚¯: {topic}ï¼‰")
        
        best_result = None
        best_distance = float('inf')
        
        for attempt in range(1, max_attempts + 1):
            print(f"\nè©¦è¡Œ {attempt}/{max_attempts}:")
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
            prompt = self._create_prompt(target_level, topic, word_count, attempt)
            
            # LLMã§ç”Ÿæˆ
            temperature = 0.65 + 0.1 * random.random()
            generated_text = self.llm.generate(prompt, max_tokens=word_count * 2, temperature=temperature)
            
            # ç‰¹å¾´é‡ã‚’æŠ½å‡º
            features = self.extractor.extract_features(generated_text)
            
            # ãƒ¬ãƒ™ãƒ«ã‚’äºˆæ¸¬ï¼ˆãƒ¢ãƒ‡ãƒ«ãŒç„¡ã„å ´åˆã¯OpenAIã§åˆ¤å®šï¼‰
            if self.predictor.model is not None:
                predicted_level, confidence = self.predictor.predict_level(features)
            else:
                predicted_level, confidence = self.llm.classify_level(generated_text)
            
            print(f"  ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ: {generated_text[:100]}...")
            print(f"  äºˆæ¸¬ãƒ¬ãƒ™ãƒ«: {predicted_level} (ä¿¡é ¼åº¦: {confidence:.2%})")
            
            # çµæœã‚’è¨˜éŒ²
            # èªæ•°ãƒã‚§ãƒƒã‚¯
            wc = len(generated_text.split())
            lower = int(round(word_count * (1 - word_tolerance)))
            upper = int(round(word_count * (1 + word_tolerance)))
            within_wc = lower <= wc <= upper

            result = {
                'text': generated_text,
                'actual_level': predicted_level,
                'target_level': target_level,
                'topic': topic,
                'confidence': confidence,
                'attempts': attempt,
                'features': features,
                'success': (predicted_level == target_level) and within_wc,
                'word_count': wc,
                'word_range': {'lower': lower, 'upper': upper}
            }
            
            # ç›®æ¨™ãƒ¬ãƒ™ãƒ«ã‹ã¤èªæ•°ãŒç¯„å›²å†…ãªã‚‰çµ‚äº†
            if result['success']:
                print(f"  âœ… ç›®æ¨™ãƒ¬ãƒ™ãƒ«ã«åˆ°é”ï¼")
                if not within_wc:
                    print(f"  âš ï¸ èªæ•°ãŒç¯„å›²å¤–: {wc} (è¨±å®¹ {lower}-{upper})")
                return result
            
            # æœ€ã‚‚è¿‘ã„çµæœã‚’ä¿å­˜
            distance = self._calculate_level_distance(predicted_level, target_level)
            if distance < best_distance:
                best_distance = distance
                best_result = result
            
            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ç”Ÿæˆã—ã¦æ¬¡ã®è©¦è¡Œã«æ´»ã‹ã™
            if attempt < max_attempts:
                reason = []
                if predicted_level != target_level:
                    reason.append("level")
                if not within_wc:
                    reason.append("word_count")
                print(f"  â†» èª¿æ•´ä¸­... ({', '.join(reason)})")
        
        print(f"\nâš ï¸ {max_attempts}å›ã®è©¦è¡Œå¾Œã€æœ€ã‚‚è¿‘ã„çµæœã‚’è¿”ã—ã¾ã™")
        return best_result
    
    def _create_prompt(self, target_level: str, topic: str, word_count: int, attempt: int) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        
        # ãƒ¬ãƒ™ãƒ«åˆ¥ã®è©³ç´°ãªæŒ‡ç¤º
        level_guidelines = {
            'A1.1': {
                'vocabulary': 'only the 500 most common English words',
                'grammar': 'present simple tense only, no complex structures',
                'sentence_length': '3-8 words per sentence',
                'example': 'I am happy. My family is big. We eat dinner together.'
            },
            'A1.2': {
                'vocabulary': 'the 750 most common English words',
                'grammar': 'present simple and continuous, basic past tense',
                'sentence_length': '5-10 words per sentence',
                'example': 'Yesterday I went to school. I am studying English now.'
            },
            'A2.2': {
                'vocabulary': 'the 1500 most common English words',
                'grammar': 'all basic tenses, simple connectors (and, but, because)',
                'sentence_length': '8-12 words per sentence',
                'example': 'I have been to Paris before because my sister lives there.'
            },
            'B1.1': {
                'vocabulary': 'common vocabulary with some less frequent words',
                'grammar': 'complex sentences, conditionals, passive voice',
                'sentence_length': '10-15 words per sentence',
                'example': 'If I had known about the meeting, I would have prepared a presentation.'
            },
            'B1.2': {
                'vocabulary': 'wider range including some abstract concepts',
                'grammar': 'all tenses, relative clauses, reported speech',
                'sentence_length': '12-18 words per sentence',
                'example': 'The manager said that the project, which had been delayed, would be completed soon.'
            },
            'B2.1': {
                'vocabulary': 'broad vocabulary including idiomatic expressions',
                'grammar': 'sophisticated structures, subjunctive mood',
                'sentence_length': '15-20 words per sentence',
                'example': 'Having considered all the options, we decided that it was essential that everyone be informed immediately.'
            },
            'B2.2': {
                'vocabulary': 'extensive vocabulary, technical terms',
                'grammar': 'all complex structures, nuanced expression',
                'sentence_length': '18-25 words per sentence',
                'example': 'The implications of this decision, while not immediately apparent, will undoubtedly have far-reaching consequences for the entire organization.'
            },
            'C1': {
                'vocabulary': 'sophisticated vocabulary, subtle distinctions',
                'grammar': 'native-like complexity and flexibility',
                'sentence_length': '20-30 words per sentence',
                'example': 'The notion that technological progress invariably leads to improved quality of life is, at best, a gross oversimplification of the complex relationship between innovation and human welfare.'
            },
            'C2': {
                'vocabulary': 'complete mastery including rare and literary forms',
                'grammar': 'full native speaker competence',
                'sentence_length': 'varied, up to 35 words',
                'example': 'One might venture to suggest that the zeitgeist of our epoch, characterized as it is by an almost pathological obsession with immediacy, militates against the cultivation of those contemplative virtues upon which genuine wisdom has traditionally been thought to depend.'
            }
        }
        
        guidelines = level_guidelines.get(target_level, level_guidelines['B1.1'])
        
        # è©¦è¡Œå›æ•°ã«å¿œã˜ã¦æŒ‡ç¤ºã‚’å¼·åŒ–
        emphasis = "STRICTLY" if attempt > 2 else ""
        lower = int(round(word_count * 0.9))
        upper = int(round(word_count * 1.1))
        
        prompt = f"""
Generate a {word_count}-word English text about "{topic}" at CEFR level {target_level}.

{emphasis} Requirements for {target_level}:
- Vocabulary: Use {guidelines['vocabulary']}
- Grammar: {guidelines['grammar']}
- Sentence length: {guidelines['sentence_length']}

Style example for {target_level}:
"{guidelines['example']}"

Important instructions:
1. Match the complexity level EXACTLY
2. Make the text natural and engaging
3. Stay strictly within the {target_level} constraints
4. Write between {lower} and {upper} words (not fewer or more)
5. Output only the text (no headings, no explanations)

Generate the text now:
"""
        
        return prompt

    def generate_n_texts(self,
                         target_level: str,
                         topic: str,
                         word_count: int,
                         n_texts: int,
                         max_attempts_each: int = 3,
                         word_tolerance: float = 0.1,
                         save_txt: bool = True,
                         save_dir: str = 'outputs/texts') -> List[Dict]:
        """æŒ‡å®šæ¡ä»¶ã§Næœ¬ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã€æ¡ä»¶ã‚’æº€ãŸã™ã‚‚ã®ã ã‘ã‚’åé›†"""
        outputs = []
        attempts_total = 0
        ts = datetime.now().strftime('%Y%m%d_%H%M%S')
        topic_slug = _slug(topic)
        level_slug = _slug(target_level)
        if save_txt:
            os.makedirs(save_dir, exist_ok=True)
        try:
            from tqdm import tqdm
            pbar = tqdm(total=n_texts, desc="Collecting", ncols=80)
        except Exception:
            pbar = None
        while len(outputs) < n_texts and attempts_total < n_texts * max_attempts_each * 2:
            res = self.generate_with_validation(
                target_level=target_level,
                topic=topic,
                word_count=word_count,
                max_attempts=max_attempts_each,
                word_tolerance=word_tolerance
            )
            attempts_total += res.get('attempts', 1)
            if res.get('success'):
                outputs.append(res)
                if save_txt:
                    idx = len(outputs)
                    fname = f"{level_slug}_{topic_slug}_{word_count}w_{ts}_{idx:02d}.txt"
                    path = os.path.join(save_dir, fname)
                    with open(path, 'w', encoding='utf-8') as f:
                        f.write(res['text'])
                if pbar:
                    pbar.update(1)
            else:
                # åé›†å¯¾è±¡å¤–ã ãŒã€è¿‘ã„çµæœã¨ã—ã¦ãƒ­ã‚°ã‚’æ®‹ã™
                pass
        if pbar:
            pbar.close()
        return outputs
    
    def _calculate_level_distance(self, level1: str, level2: str) -> float:
        """ãƒ¬ãƒ™ãƒ«é–“ã®è·é›¢ã‚’è¨ˆç®—"""
        level_order = ['A1.1', 'A1.2', 'A2.2', 'B1.1', 'B1.2', 'B2.1', 'B2.2', 'C1', 'C2']
        
        try:
            idx1 = level_order.index(level1)
            idx2 = level_order.index(level2)
            return abs(idx1 - idx2)
        except ValueError:
            return float('inf')
    
    def batch_generate(self, 
                       levels: List[str],
                       topics: List[str],
                       word_count: int = 200,
                       save_txt: bool = True,
                       save_dir: str = 'outputs/texts') -> List[Dict]:
        """è¤‡æ•°ã®ãƒ¬ãƒ™ãƒ«ãƒ»ãƒˆãƒ”ãƒƒã‚¯ã§ä¸€æ‹¬ç”Ÿæˆã€‚å¿…è¦ã«å¿œã˜ã¦.txtä¿å­˜ã€‚"""
        
        results = []
        total = len(levels) * len(topics)
        ts = datetime.now().strftime('%Y%m%d_%H%M%S')
        if save_txt:
            os.makedirs(save_dir, exist_ok=True)
        print(f"\nğŸ“š {total}å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã™...")
        
        try:
            from tqdm import tqdm
            iterator = tqdm(itertools.product(levels, topics), total=total, desc="Generating", ncols=80)
        except Exception:
            iterator = itertools.product(levels, topics)
        
        count = 0
        for level, topic in iterator:
            count += 1
            result = self.generate_with_validation(
                target_level=level,
                topic=topic,
                word_count=word_count,
                max_attempts=3
            )
            results.append(result)
            if save_txt and result.get('success'):
                fname = f"{_slug(level)}_{_slug(topic)}_{word_count}w_{ts}_{count:02d}.txt"
                path = os.path.join(save_dir, fname)
                with open(path, 'w', encoding='utf-8') as f:
                    f.write(result['text'])
        
        # çµæœã‚’DataFrameã«ã¾ã¨ã‚ã‚‹
        df_results = pd.DataFrame(results)
        success_rate = df_results['success'].mean()
        
        print(f"\nğŸ“Š ç”Ÿæˆå®Œäº†ï¼")
        print(f"  æˆåŠŸç‡: {success_rate:.1%}")
        print(f"  å¹³å‡ä¿¡é ¼åº¦: {df_results['confidence'].mean():.1%}")
        print(f"  å¹³å‡è©¦è¡Œå›æ•°: {df_results['attempts'].mean():.1f}")
        
        return results

# ========================================
# STEP 6: å®Ÿè¡Œä¾‹ã¨ãƒ†ã‚¹ãƒˆ
# ========================================

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("=" * 70)
    print("ğŸ“ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å‹è‹±æ–‡ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ  - å®Ÿè¡Œãƒ‡ãƒ¢")
    print("=" * 70)
    
    # Excelãƒ•ã‚¡ã‚¤ãƒ«ã¨CEFR-J Wordlistã®ãƒ‘ã‚¹ï¼ˆçµ¶å¯¾ãƒ‘ã‚¹ã‚’ã‚³ãƒ¼ãƒ‰ã«çµ„ã¿è¾¼ã¿ï¼‰
    excel_path = DEFAULT_EXCEL_PATH
    
    # CEFR-J Wordlistã®ãƒ‘ã‚¹
    default_wordlist = os.getenv('CEFR_WORDLIST_PATH', '').strip()
    cefr_wordlist_path = DEFAULT_WORDLIST_PATH
    if not os.path.exists(excel_path):
        print(f"âš ï¸ å­¦ç¿’ExcelãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {excel_path}")
    if not os.path.exists(cefr_wordlist_path):
        print(f"âš ï¸ CEFR-J WordlistãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {cefr_wordlist_path}")

    # OpenAI APIã‚­ãƒ¼ã®å…¥åŠ›
    print("\nğŸ”‘ OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    print("ï¼ˆç’°å¢ƒå¤‰æ•°ã«è¨­å®šæ¸ˆã¿ã®å ´åˆã¯Enterã‚­ãƒ¼ã‚’æŠ¼ã—ã¦ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
    api_key = input("APIã‚­ãƒ¼: ").strip() or None
    
    # ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    print("\nğŸ”§ ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ä¸­...")
    generator = HybridTextGenerator(
        excel_path=excel_path,
        api_key=api_key,
        cefr_wordlist_path=cefr_wordlist_path
    )
    
    # ãƒ†ã‚¹ãƒˆç”Ÿæˆ
    print("\n" + "=" * 70)
    print("ğŸ“ ãƒ†ã‚¹ãƒˆç”Ÿæˆã‚’å®Ÿè¡Œ")
    print("=" * 70)
    print("å¯¾å¿œCEFRãƒ¬ãƒ™ãƒ«: A1.1, A1.2, A2.2, B1.1, B1.2, B2.1, B2.2, C1, C2")
    print("ä¾‹ãƒˆãƒ”ãƒƒã‚¯: Environmental Protection, Technology, Education, Health, Culture, Travel, Science, Sports, Food, Society")
    
    # å˜ä¸€ç”Ÿæˆã®ä¾‹
    result = generator.generate_with_validation(
        target_level='B1.1',
        topic='Environmental Protection',
        word_count=150,
        max_attempts=3
    )
    
    print("\n" + "=" * 50)
    print("ğŸ“„ ç”Ÿæˆçµæœ:")
    print("=" * 50)
    print(f"ç›®æ¨™ãƒ¬ãƒ™ãƒ«: {result['target_level']}")
    print(f"å®Ÿéš›ã®ãƒ¬ãƒ™ãƒ«: {result['actual_level']}")
    print(f"ä¿¡é ¼åº¦: {result['confidence']:.1%}")
    print(f"å˜èªæ•°: {result['word_count']}")
    print(f"æˆåŠŸ: {'âœ…' if result['success'] else 'âŒ'}")
    print(f"\nç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ:\n{result['text']}")
    # å˜ä½“å‡ºåŠ›ã‚’ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä¿å­˜
    ts1 = datetime.now().strftime('%Y%m%d_%H%M%S')
    first_txt_dir = os.path.join('outputs', 'texts')
    os.makedirs(first_txt_dir, exist_ok=True)
    first_fname = f"{_slug(result['target_level'])}_{_slug(result['topic'])}_{result['word_count']}w_{ts1}.txt"
    with open(os.path.join(first_txt_dir, first_fname), 'w', encoding='utf-8') as f:
        f.write(result['text'])
    print(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆã‚’ outputs/texts/{first_fname} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    # ä»»æ„: æŒ‡å®šæœ¬æ•°ã®ã‚«ã‚¹ã‚¿ãƒ ç”Ÿæˆ
    do_custom = input("\næŒ‡å®šæœ¬æ•°ã®ã‚«ã‚¹ã‚¿ãƒ ç”Ÿæˆã‚’å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/N): ").strip().lower() == 'y'
    if do_custom:
        level_in = input("CEFRãƒ¬ãƒ™ãƒ«ã‚’å…¥åŠ›ï¼ˆä¾‹: A1.1/B1.1/C1ï¼‰: ").strip() or 'B1.1'
        topic_in = input("ãƒˆãƒ”ãƒƒã‚¯ï¼ˆä¾‹: Technologyï¼‰: ").strip() or 'General Topic'
        try:
            wc_in = int(input("èªæ•°ï¼ˆä¾‹: 120ï¼‰: ").strip() or '120')
        except ValueError:
            wc_in = 120
        try:
            n_in = int(input("ç”Ÿæˆæœ¬æ•°ï¼ˆä¾‹: 5ï¼‰: ").strip() or '5')
        except ValueError:
            n_in = 5
        print("\nğŸš€ æŒ‡å®šæ¡ä»¶ã§ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™...")
        items = generator.generate_n_texts(
            target_level=level_in,
            topic=topic_in,
            word_count=wc_in,
            n_texts=n_in,
            max_attempts_each=3,
            word_tolerance=0.1,
            save_txt=True
        )
        df_custom = pd.DataFrame(items)
        ts = datetime.now().strftime('%Y%m%d_%H%M%S')
        out_custom = os.path.join('outputs', f'custom_texts_{ts}.csv')
        os.makedirs(os.path.dirname(out_custom), exist_ok=True)
        df_custom.to_csv(out_custom, index=False)
        print(f"âœ… ã‚«ã‚¹ã‚¿ãƒ ç”Ÿæˆã®çµæœã‚’ {out_custom} ã«ä¿å­˜ã—ã¾ã—ãŸ")

    # ãƒãƒƒãƒç”Ÿæˆã®ä¾‹
    print("\n" + "=" * 70)
    print("ğŸ“š ãƒãƒƒãƒç”Ÿæˆã®ãƒ‡ãƒ¢")
    print("=" * 70)
    
    batch_results = generator.batch_generate(
        levels=['A1.1','A1.2','A2.2','B1.1','B1.2','B2.1','B2.2','C1','C2'],
        topics=['My Daily Routine','Technology','Education','Health','Travel'],
        word_count=100,
        save_txt=True,
        save_dir=os.path.join('outputs', 'texts')
    )
    
    # çµæœã®ä¿å­˜
    print("\nğŸ’¾ çµæœã‚’ä¿å­˜ä¸­...")
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
    df_results = pd.DataFrame(batch_results)
    output_path = os.path.join('outputs', 'generation_results.csv')
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    df_results.to_csv(output_path, index=False)
    print(f"âœ… çµæœã‚’ {output_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    model_path = os.path.join('outputs', 'readability_model.pkl')
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    generator.predictor.save_model(model_path)
    
    print("\n" + "=" * 70)
    print("âœ¨ å®Œäº†ï¼")
    print("=" * 70)
    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. ã‚ˆã‚Šå¤šãã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´")
    print("2. ç‰¹å¾´é‡æŠ½å‡ºã®ç²¾åº¦å‘ä¸Š")
    print("3. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœ€é©åŒ–")
    print("4. Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åŒ–")

if __name__ == "__main__":
    # å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
    print("""
    ============================================
    å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    ============================================
    
    # åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
    pip install pandas numpy scikit-learn openpyxl
    
    # ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
    pip install textstat spacy wordfreq
    python -m spacy download en_core_web_sm
    
    # LLM API
    pip install openai        # OpenAI ã®ã¿ä½¿ç”¨
    
    ============================================
    """)
    
    # ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
    main()
