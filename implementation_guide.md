# è‹±æ–‡èª­ã¿ã‚„ã™ã•ãƒ¬ãƒ™ãƒ«æŒ‡å®šãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ  - å®Ÿè£…ã‚¬ã‚¤ãƒ‰

## ğŸ“Š ç¾çŠ¶åˆ†æ

ã‚ãªãŸã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼š
- **30å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«** (Text_1.txt ~ Text_30.txt)
- **8ã¤ã®èª­ã¿ã‚„ã™ã•æŒ‡æ¨™**ï¼š
  - AvrDiff: å¹³å‡é›£æ˜“åº¦
  - BperA: æ–‡ç« ã®è¤‡é›‘ã•
  - CVV1: èªå½™ã®å¤šæ§˜æ€§
  - AvrFreqRank: å¹³å‡é »åº¦ãƒ©ãƒ³ã‚¯
  - ARI: Automated Readability Index
  - VperSent: æ–‡ã‚ãŸã‚Šã®å‹•è©æ•°
  - POStypes: å“è©ã‚¿ã‚¤ãƒ—æ•°
  - LenNP: åè©å¥ã®é•·ã•
- **9ã¤ã®CEFRãƒ¬ãƒ™ãƒ«**: A1.1, A1.2, A2.2, B1.1, B1.2, B2.1, B2.2, C1, C2

## ğŸ¯ å®Ÿç¾å¯èƒ½ãª4ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

### ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ä¸»å°å‹
**æ¦‚è¦**: LLMï¼ˆGPT-4ç­‰ï¼‰ã«è©³ç´°ãªåˆ¶ç´„ã‚’ä¸ãˆã¦ç”Ÿæˆ

```mermaid
graph LR
    A[ç›®æ¨™ãƒ¬ãƒ™ãƒ«è¨­å®š] --> B[ãƒ¬ãƒ™ãƒ«ç‰¹å¾´æŠ½å‡º]
    B --> C[è©³ç´°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ]
    C --> D[LLMç”Ÿæˆ]
    D --> E[èª­ã¿ã‚„ã™ã•æ¤œè¨¼]
    E --> F{åŸºæº–æº€ãŸã™?}
    F -->|Yes| G[å®Œæˆ]
    F -->|No| C
```

**ãƒ¡ãƒªãƒƒãƒˆ**:
- å®Ÿè£…ãŒæ¯”è¼ƒçš„ç°¡å˜
- è‡ªç„¶ãªæ–‡ç« ç”Ÿæˆ
- ãƒˆãƒ”ãƒƒã‚¯ã®æŸ”è»Ÿæ€§ãŒé«˜ã„

**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**:
- APIåˆ©ç”¨ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹
- å³å¯†ãªåˆ¶å¾¡ãŒé›£ã—ã„
- å†ç¾æ€§ã«èª²é¡Œ

**å®Ÿè£…ã‚¹ãƒ†ãƒƒãƒ—**:
```python
# ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¬ãƒ™ãƒ«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
level_profiles = {
    'A1.1': {
        'sentence_length': (3, 8),
        'vocabulary_size': 500,
        'grammar': ['present simple'],
        'topics': ['daily life', 'family', 'basic needs']
    },
    # ä»–ã®ãƒ¬ãƒ™ãƒ«ã‚‚åŒæ§˜ã«å®šç¾©
}

# ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
def create_prompt(level, topic, word_count):
    profile = level_profiles[level]
    prompt = f"""
    Generate a {word_count}-word English text about "{topic}".
    
    Requirements for {level} level:
    - Sentence length: {profile['sentence_length'][0]}-{profile['sentence_length'][1]} words
    - Use only: {', '.join(profile['grammar'])}
    - Vocabulary: top {profile['vocabulary_size']} most common words
    - Style: Simple, clear, concrete examples
    
    Text:
    """
    return prompt
```

### ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: åå¾©æ”¹å–„å‹
**æ¦‚è¦**: ç”Ÿæˆã¨è©•ä¾¡ã‚’ç¹°ã‚Šè¿”ã—ã¦ç›®æ¨™ãƒ¬ãƒ™ãƒ«ã«åæŸ

```mermaid
graph TD
    A[åˆæœŸç”Ÿæˆ] --> B[èª­ã¿ã‚„ã™ã•åˆ†æ]
    B --> C{ç›®æ¨™ã¨ã®å·®åˆ†è¨ˆç®—}
    C -->|å·®åˆ†å¤§| D[æ”¹å–„æŒ‡ç¤ºç”Ÿæˆ]
    D --> E[ãƒ†ã‚­ã‚¹ãƒˆä¿®æ­£]
    E --> B
    C -->|å·®åˆ†å°| F[å®Œæˆ]
```

**ãƒ¡ãƒªãƒƒãƒˆ**:
- ç²¾åº¦ã®é«˜ã„åˆ¶å¾¡ãŒå¯èƒ½
- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã®ä¸€è‡´åº¦ãŒé«˜ã„
- æ”¹å–„éç¨‹ãŒè¿½è·¡å¯èƒ½

**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**:
- å‡¦ç†æ™‚é–“ãŒé•·ã„
- è¤‡æ•°å›ã®APIå‘¼ã³å‡ºã—ãŒå¿…è¦
- è¤‡é›‘ãªå®Ÿè£…

**å®Ÿè£…ä¾‹**:
```python
def iterative_generation(target_level, topic, max_iterations=5):
    text = initial_generation(topic)
    
    for i in range(max_iterations):
        metrics = calculate_metrics(text)
        distance = calculate_distance(metrics, target_level)
        
        if distance < threshold:
            break
            
        adjustments = determine_adjustments(metrics, target_level)
        text = apply_adjustments(text, adjustments)
    
    return text
```

### ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ3: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å‹ï¼ˆæ¨å¥¨ï¼‰
**æ¦‚è¦**: æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¨LLMã‚’çµ„ã¿åˆã‚ã›ãŸç”Ÿæˆ

```mermaid
graph TB
    A[æ—¢å­˜ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ] --> B[MLãƒ¢ãƒ‡ãƒ«è¨“ç·´]
    B --> C[ãƒ¬ãƒ™ãƒ«äºˆæ¸¬å™¨]
    D[ãƒˆãƒ”ãƒƒã‚¯å…¥åŠ›] --> E[åˆæœŸç”Ÿæˆ]
    E --> F[ç‰¹å¾´é‡æŠ½å‡º]
    F --> C
    C --> G{ãƒ¬ãƒ™ãƒ«åˆ¤å®š}
    G -->|ä¸ä¸€è‡´| H[èª¿æ•´ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ]
    H --> I[å†ç”Ÿæˆ]
    I --> F
    G -->|ä¸€è‡´| J[æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆ]
```

**å®Ÿè£…ãƒ•ãƒ­ãƒ¼**:

```python
# ã‚¹ãƒ†ãƒƒãƒ—1: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
import textstat

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
df = pd.read_excel('readability_data.xlsx')
texts = load_text_files()  # Text_1.txt ~ Text_30.txt ã‚’èª­ã¿è¾¼ã¿

# ç‰¹å¾´é‡æŠ½å‡ºé–¢æ•°
def extract_features(text):
    return {
        'ari': textstat.automated_readability_index(text),
        'fre': textstat.flesch_reading_ease(text),
        'word_count': len(text.split()),
        'avg_sentence_length': textstat.avg_sentence_length(text),
        # ä»–ã®ç‰¹å¾´é‡ã‚‚è¿½åŠ 
    }

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
features = [extract_features(text) for text in texts]
model = RandomForestClassifier()
model.fit(features, df['predict_level'])

# ã‚¹ãƒ†ãƒƒãƒ—2: ç”Ÿæˆã¨æ¤œè¨¼ã®ãƒ«ãƒ¼ãƒ—
def generate_validated_text(target_level, topic):
    attempts = 0
    while attempts < 10:
        # LLMã§ç”Ÿæˆ
        text = llm_generate(target_level, topic)
        
        # ãƒ¬ãƒ™ãƒ«äºˆæ¸¬
        features = extract_features(text)
        predicted_level = model.predict([features])[0]
        
        if predicted_level == target_level:
            return text
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä½œæˆ
        feedback = create_feedback(features, target_level)
        attempts += 1
    
    return text  # æœ€å¾Œã®è©¦è¡Œã‚’è¿”ã™
```

### ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ4: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ‹¡å¼µå‹
**æ¦‚è¦**: ãƒ¬ãƒ™ãƒ«åˆ¥ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’LLMã§æ‹¡å¼µ

**ãƒ¡ãƒªãƒƒãƒˆ**:
- ç¢ºå®Ÿãªãƒ¬ãƒ™ãƒ«åˆ¶å¾¡
- ä½ã‚³ã‚¹ãƒˆ
- æ•™è‚²ç”¨é€”ã«æœ€é©

**å®Ÿè£…ä¾‹**:
```python
class TemplateExpander:
    def __init__(self):
        self.templates = {
            'A1.1': [
                "{subject} {be} {adjective}.",
                "{subject} {have} {object}.",
                "{subject} {action} every day."
            ],
            'B1.1': [
                "Although {clause1}, {clause2}.",
                "If {condition}, {result}.",
                "{subject} {past_action} because {reason}."
            ]
        }
    
    def expand_with_llm(self, level, topic):
        template = random.choice(self.templates[level])
        
        # LLMã§ã‚¹ãƒ­ãƒƒãƒˆã‚’åŸ‹ã‚ã‚‹
        prompt = f"""
        Fill in this template about {topic}:
        Template: {template}
        Use vocabulary appropriate for {level} level.
        """
        
        filled = llm_complete(prompt)
        return filled
```

## ğŸ› ï¸ å®Ÿè£…ã«å¿…è¦ãªãƒ„ãƒ¼ãƒ«ã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

### å¿…é ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª
```bash
# åŸºæœ¬çš„ãªå‡¦ç†
pip install pandas numpy openpyxl

# ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
pip install textstat
pip install spacy
python -m spacy download en_core_web_sm
pip install nltk

# æ©Ÿæ¢°å­¦ç¿’
pip install scikit-learn

# LLMé€£æº
pip install openai  # OpenAI APIä½¿ç”¨ã®å ´åˆ
pip install anthropic  # Claude APIä½¿ç”¨ã®å ´åˆ
pip install transformers  # ãƒ­ãƒ¼ã‚«ãƒ«LLMä½¿ç”¨ã®å ´åˆ
```

### èª­ã¿ã‚„ã™ã•æŒ‡æ¨™ã®è¨ˆç®—
```python
import textstat
import spacy
from collections import Counter

nlp = spacy.load("en_core_web_sm")

def calculate_readability_metrics(text):
    doc = nlp(text)
    
    # åŸºæœ¬æŒ‡æ¨™
    ari = textstat.automated_readability_index(text)
    
    # èªå½™ã®å¤šæ§˜æ€§ (CVV1)
    tokens = [token.text.lower() for token in doc if token.is_alpha]
    cvv1 = len(set(tokens)) / len(tokens) if tokens else 0
    
    # æ–‡ã‚ãŸã‚Šã®å‹•è©æ•°
    verbs = [token for token in doc if token.pos_ == "VERB"]
    sentences = list(doc.sents)
    v_per_sent = len(verbs) / len(sentences) if sentences else 0
    
    # åè©å¥ã®å¹³å‡é•·
    noun_phrases = [chunk for chunk in doc.noun_chunks]
    avg_np_length = sum(len(np.text.split()) for np in noun_phrases) / len(noun_phrases) if noun_phrases else 0
    
    return {
        'ARI': ari,
        'CVV1': cvv1,
        'VperSent': v_per_sent,
        'LenNP': avg_np_length,
        # ä»–ã®æŒ‡æ¨™ã‚‚è¨ˆç®—
    }
```

## ğŸ“‹ æ®µéšçš„å®Ÿè£…è¨ˆç”»

### Phase 1: ãƒ‡ãƒ¼ã‚¿åˆ†æã¨æº–å‚™ï¼ˆ1-2æ—¥ï¼‰
1. 30å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
2. å„ãƒ†ã‚­ã‚¹ãƒˆã®å®Ÿéš›ã®æŒ‡æ¨™ã‚’è¨ˆç®—
3. Excelãƒ‡ãƒ¼ã‚¿ã¨ã®æ•´åˆæ€§ç¢ºèª
4. ãƒ¬ãƒ™ãƒ«åˆ¥ã®ç‰¹å¾´ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ

### Phase 2: åŸºæœ¬ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰ï¼ˆ2-3æ—¥ï¼‰
1. LLM APIã®è¨­å®šï¼ˆOpenAI/Claude/ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
2. åŸºæœ¬çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆæ©Ÿèƒ½
3. èª­ã¿ã‚„ã™ã•åˆ†ææ©Ÿèƒ½ã®å®Ÿè£…
4. ç°¡å˜ãªãƒ¬ãƒ™ãƒ«ï¼ˆA1.1, A1.2ï¼‰ã§ãƒ†ã‚¹ãƒˆ

### Phase 3: æ”¹å–„ã¨æœ€é©åŒ–ï¼ˆ3-4æ—¥ï¼‰
1. åå¾©æ”¹å–„ãƒ«ãƒ¼ãƒ—ã®å®Ÿè£…
2. MLãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒ¬ãƒ™ãƒ«äºˆæ¸¬å™¨
3. è¤‡æ•°æ‰‹æ³•ã®çµ±åˆ
4. è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰

### Phase 4: æ¤œè¨¼ã¨èª¿æ•´ï¼ˆ2-3æ—¥ï¼‰
1. ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã®å“è³ªè©•ä¾¡
2. äººé–“ã«ã‚ˆã‚‹è©•ä¾¡ã¨ã®æ¯”è¼ƒ
3. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
4. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ

## ğŸ’¡ å®Ÿè£…ã®ã‚³ãƒ„ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
```python
# ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨æ•´ç†
import os

def load_all_texts(directory):
    texts = {}
    for i in range(1, 31):
        filename = f"Text_{i}.txt"
        filepath = os.path.join(directory, filename)
        if os.path.exists(filepath):
            with open(filepath, 'r', encoding='utf-8') as f:
                texts[f"Text_{i}"] = f.read()
    return texts
```

### 2. ãƒ¬ãƒ™ãƒ«é–“ã®å¢ƒç•Œã‚’æ˜ç¢ºã«
```python
# ãƒ¬ãƒ™ãƒ«å¢ƒç•Œã®å®šç¾©
level_boundaries = {
    'A1.1': {'ARI': (0, 2), 'word_count': (50, 100)},
    'A1.2': {'ARI': (2, 3), 'word_count': (100, 150)},
    'A2.2': {'ARI': (3, 5), 'word_count': (150, 200)},
    # ... ç¶šã
}
```

### 3. è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å®Ÿè£…
```python
def evaluate_generation(generated_text, target_level, reference_texts):
    """
    ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã®å“è³ªã‚’è©•ä¾¡
    """
    metrics = calculate_readability_metrics(generated_text)
    
    # ç›®æ¨™ãƒ¬ãƒ™ãƒ«ã¨ã®é©åˆåº¦
    level_match_score = calculate_level_match(metrics, target_level)
    
    # è‡ªç„¶ã•ã®ã‚¹ã‚³ã‚¢ï¼ˆperplexityç­‰ï¼‰
    naturalness_score = calculate_naturalness(generated_text)
    
    # å†…å®¹ã®ä¸€è²«æ€§
    coherence_score = calculate_coherence(generated_text)
    
    return {
        'level_match': level_match_score,
        'naturalness': naturalness_score,
        'coherence': coherence_score,
        'overall': (level_match_score + naturalness_score + coherence_score) / 3
    }
```

## ğŸ” ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºç­–

1. **ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã®ãƒ¬ãƒ™ãƒ«ãŒå®‰å®šã—ãªã„**
   - è§£æ±º: åå¾©å›æ•°ã‚’å¢—ã‚„ã™ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚ˆã‚Šè©³ç´°ã«

2. **è¨ˆç®—ã—ãŸæŒ‡æ¨™ã¨ç›®æ¨™ãŒä¸€è‡´ã—ãªã„**
   - è§£æ±º: ç‰¹å¾´é‡ã®é‡ã¿ä»˜ã‘ã‚’èª¿æ•´ã€æ­£è¦åŒ–ã‚’é©ç”¨

3. **ç”Ÿæˆé€Ÿåº¦ãŒé…ã„**
   - è§£æ±º: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å®Ÿè£…ã€ãƒãƒƒãƒå‡¦ç†ã®å°å…¥

4. **ä¸è‡ªç„¶ãªæ–‡ç« ãŒç”Ÿæˆã•ã‚Œã‚‹**
   - è§£æ±º: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æ”¹å–„ã€å¾Œå‡¦ç†ã®è¿½åŠ 

## ğŸ“š å‚è€ƒãƒªã‚½ãƒ¼ã‚¹

- [CEFR Level Descriptions](https://www.coe.int/en/web/common-european-framework-reference-languages)
- [Textstat Documentation](https://pypi.org/project/textstat/)
- [spaCy Documentation](https://spacy.io/usage/linguistic-features)
- [OpenAI API Documentation](https://platform.openai.com/docs)

## ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. ã¾ãšæ—¢å­˜ã®30å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ
2. æœ€ã‚‚ç°¡å˜ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ™ãƒ¼ã‚¹ï¼‰ã‹ã‚‰å®Ÿè£…
3. æ®µéšçš„ã«æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¦ã„ã
4. è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¦å“è³ªã‚’ç¢ºèª

ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã§ã€ä»»æ„ã®CEFRãƒ¬ãƒ™ãƒ«ã«å¯¾å¿œã—ãŸè‹±æ–‡ã‚’è‡ªå‹•ç”Ÿæˆã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
